{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f55fdd8",
   "metadata": {},
   "source": [
    "# Intro to LangChain\n",
    "LangChain is a popular framework that allows you to quickly build applications and pipelines of Large Language Models (LLMs). You can use it to create chatbots, RAGs, agents and much more.\n",
    "\n",
    "The main idea of the library is that we can create a _chain_ of different components to create more complex applications. These _chains_ (you can think of them as pipelines) can be made up of various components such as:\n",
    "- **Prompts templates**: Prompts templates are templates to generate different type of prompts. Like chat prompts, question answering prompts, etc.\n",
    "- **LLMs**: Large Language Models are the core of LangChain. You can use any LLM that is compatible with the library, like OpenAI, Hugging Face, LLama, etc.\n",
    "- **Tools**: Tools are functions that can be used by the LLM to perform specific tasks. For example, you can use a tool to search the web, or to access a database.\n",
    "- **Agents**: Agents are components that can use LLMs and tools to perform specific tasks. They can be used to create chatbots, **R**etrieval **A**ugumentation **G**eneration (RAGs), etc.\n",
    "- **Retrievers**: Retrievers are components that can be used to retrieve information from a database or a knowledge base. They can be used to create RAGs, or to retrieve information from a database.  \n",
    "- **Memory**: Memory is a component that can be used to store information about the conversation. It can be used to create chatbots that can remember previous conversations, or to create RAGs that can remember previous queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f782a95",
   "metadata": {},
   "source": [
    "## Using LLMs in LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0d041d",
   "metadata": {},
   "source": [
    "LangChain supports a wide range of providers for LLMs, including OpenAI, Hugging Face, Groq,  LLama and many others.\n",
    "\n",
    "Let's start our exploration of LangChain by using Grog integration. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb61e82",
   "metadata": {},
   "source": [
    "### Groq Integration\n",
    "Groq is a provider of LLMs that offers high-performance inference capabilities. To use Groq with LangChain, you need to set up your API key in the `.env` file. Follow the steps in the README.md file to set up your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80c0774",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab9b45a",
   "metadata": {},
   "source": [
    "#### Load Credentials from .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8777880",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bdf171",
   "metadata": {},
   "source": [
    "#### Defining the LLM (Using Groq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44f8df8",
   "metadata": {},
   "source": [
    "We can define the LLM using the [`ChatGroq`](https://python.langchain.com/docs/integrations/chat/groq/) class from the `langchain_groq` module. \n",
    "This class allows us to specify:\n",
    "+ the model - below we use `llama-3.1-8b-instant`\n",
    "+ the temperature - we set it to `0.1` for more deterministic responses\n",
    "+ the maximum tokens - we set it to `512` to limit the response length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e42b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfad1d0",
   "metadata": {},
   "source": [
    "#### Build prompt template\n",
    "A prompt is a set of instructions or input provided by a user to an LLM to guide its response. It helps the model understand the context and generate relevant output. In LangChain, we can create a prompt template using the `PromptTemplate` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ddde28",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0c0a9d",
   "metadata": {},
   "source": [
    "The __input_variables__ are defined in the template using curly braces '{}'. This allows us to dynamically insert values into the template when we use it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1884340c",
   "metadata": {},
   "source": [
    "#### Define Chain\n",
    "A chain is sequence of components that are executed in order to produce a final output. In LangChain, we can use the pipe symbol `|` to define a chain of components. The output of one component is passed as input to the next component in the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6cc0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b708f0f",
   "metadata": {},
   "source": [
    "#### Invoke the Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e07785",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the backpropagation algorithm?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c1daa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = chain.invoke(input={\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c5f486",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer.content.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3203d797",
   "metadata": {},
   "source": [
    "If we'd like to ask multiple questions we can by passing a list of dictionary objects, where the dictionaries must contain the input variable set in our prompt template (\"question\") that is mapped to the question we'd like to ask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c306b4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "qs = [ \n",
    "    {\"question\": \"What is the backpropagation algorithm?\"},\n",
    "    {\"question\": \"What is the purpose of the activation function in a neural network?\"},\n",
    "    {\"question\": \"What is the difference between supervised and unsupervised learning?\"},\n",
    "    {\"question\": \"Explain the concept of overfitting in machine learning.\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4f0f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = chain.batch(qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722097ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for question, answer in zip(qs, answers):\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"Question: {question['question']}\")\n",
    "    print(f\"Answer: {answer.content.strip()}\")\n",
    "    print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05701d8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
